@inproceedings{murtagh2015complexity,
  title={The complexity of computing the optimal composition of differential privacy},
  author={Murtagh, Jack and Vadhan, Salil},
  booktitle={Theory of Cryptography Conference},
  pages={157--175},
  year={2015},
  organization={Springer}
}

@inproceedings{kairouz2015composition,
  title={The composition theorem for differential privacy},
  author={Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
  booktitle={International conference on machine learning},
  pages={1376--1385},
  year={2015},
  organization={PMLR}
}
@unpublished{DRS19,
  ids = {DRS19a,dong2019gaussian},
  title = {Gaussian {{Differential Privacy}}},
  author = {Dong, Jinshuo and Roth, Aaron and Su, Weijie J.},
  date = {2019-05-30},
  eprint = {1905.02383},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.02383},
  urldate = {2020-08-04},
  abstract = {In the past decade, differential privacy has seen remarkable success as a rigorous and practical formalization of data privacy. This privacy definition and its divergence based relaxations, however, have several acknowledged weaknesses, either in handling composition of private algorithms or in analyzing important primitives like privacy amplification by subsampling. Inspired by the hypothesis testing formulation of privacy, this paper proposes a new relaxation of differential privacy, which we term ``f -differential privacy'' (f -DP). This notion of privacy has a number of appealing properties and, in particular, avoids difficulties associated with divergence based relaxations. First, f -DP faithfully preserves the hypothesis testing interpretation of differential privacy, thereby making the privacy guarantees easily interpretable. In addition, f -DP allows for lossless reasoning about composition in an algebraic fashion. Moreover, we provide a powerful technique to import existing results proven for the original differential privacy definition to f -DP and, as an application of this technique, obtain a simple and easy-to-interpret theorem of privacy amplification by subsampling for f -DP.},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{DRS22,
  ids = {dong2022gaussian},
  title = {Gaussian {{Differential Privacy}}},
  author = {Dong, Jinshuo and Roth, Aaron and Su, Weijie J.},
  date = {2022-02-01},
  journaltitle = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  shortjournal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {84},
  number = {1},
  pages = {3--37},
  issn = {1369-7412},
  doi = {10.1111/rssb.12454},
  url = {https://doi.org/10.1111/rssb.12454},
  urldate = {2023-11-15},
  abstract = {In the past decade, differential privacy has seen remarkable success as a rigorous and practical formalization of data privacy. This privacy definition and its divergence based relaxations, however, have several acknowledged weaknesses, either in handling composition of private algorithms or in analysing important primitives like privacy amplification by subsampling. Inspired by the hypothesis testing formulation of privacy, this paper proposes a new relaxation of differential privacy, which we term `f-differential privacy' (f-DP). This notion of privacy has a number of appealing properties and, in particular, avoids difficulties associated with divergence based relaxations. First, f-DP faithfully preserves the hypothesis testing interpretation of differential privacy, thereby making the privacy guarantees easily interpretable. In addition, f-DP allows for lossless reasoning about composition in an algebraic fashion. Moreover, we provide a powerful technique to import existing results proven for the original differential privacy definition to f-DP and, as an application of this technique, obtain a simple and easy-to-interpret theorem of privacy amplification by subsampling for f-DP. In addition to the above findings, we introduce a canonical single-parameter family of privacy notions within the f-DP class that is referred to as `Gaussian differential privacy' (GDP), defined based on hypothesis testing of two shifted Gaussian distributions. GDP is the focal privacy definition among the family of f-DP guarantees due to a central limit theorem for differential privacy that we prove. More precisely, the privacy guarantees of any hypothesis testing based definition of privacy (including the original differential privacy definition) converges to GDP in the limit under composition. We also prove a Berry--Esseen style version of the central limit theorem, which gives a computationally inexpensive tool for tractably analysing the exact composition of private algorithms. Taken together, this collection of attractive properties render f-DP a mathematically coherent, analytically tractable and versatile framework for private data analysis. Finally, we demonstrate the use of the tools we develop by giving an improved analysis of the privacy guarantees of noisy stochastic gradient descent.}
}

@inproceedings{FZ21,
  ids = {feldman2021individual},
  title = {Individual Privacy Accounting via a R\'enyi Filter},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Feldman, Vitaly and Zrnic, Tijana},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  date = {2021},
  volume = {34},
  pages = {28080--28091},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/ec7f346604f518906d35ef0492709f78-Paper.pdf}
}

@inproceedings{KTH22,
  ids = {koskelaindividual},
  title = {Individual {{Privacy Accounting}} with {{Gaussian Differential Privacy}}},
  author = {Koskela, Antti and Tobaben, Marlon and Honkela, Antti},
  date = {2022-09-29},
  url = {https://openreview.net/forum?id=JmC_Tld3v-f},
  urldate = {2025-10-24},
  abstract = {Individual privacy accounting enables bounding differential privacy (DP) loss individually for each participant involved in the analysis. This can be informative as often the individual privacy losses are considerably smaller than those indicated by the DP bounds that are based on considering worst-case bounds at each data access. In order to account for the individual losses in a principled manner, we need a privacy accountant for adaptive compositions of mechanisms, where the loss incurred at a given data access is allowed to be smaller than the worst-case loss. This kind of analysis has been carried out for the R\'enyi differential privacy by Feldman and Zrnic (2021), however not yet for the so-called optimal privacy accountants. We make first steps in this direction by providing a careful analysis using the Gaussian differential privacy which gives optimal bounds for the Gaussian mechanism, one of the most versatile DP mechanisms. This approach is based on determining a certain supermartingale for the hockey-stick divergence and on extending the R\'enyi divergence-based fully adaptive composition results by Feldman and Zrnic (2021). We also consider measuring the individual \$(\textbackslash varepsilon,\textbackslash delta)\$-privacy losses using the so-called privacy loss distributions. Using the Blackwell theorem, we can then use the results of Feldman and Zrnic (2021) to construct an approximative individual \$(\textbackslash varepsilon,\textbackslash delta)\$-accountant. We also show how to speed up the FFT-based individual DP accounting using the Plancherel theorem.},
  eventtitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  langid = {english}
}

@inproceedings{RRUV16,
  ids = {filters_roger,rogers2016privacy},
  title = {Privacy Odometers and Filters: {{Pay-as-you-go}} Composition},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Rogers, Ryan M and Roth, Aaron and Ullman, Jonathan and Vadhan, Salil},
  editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
  date = {2016},
  volume = {29},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/58c54802a9fb9526cd0923353a34a7ae-Paper.pdf}
}

@online{ST22,
  ids = {smith2022fullyadaptivecompositiongaussian},
  title = {Fully {{Adaptive Composition}} for {{Gaussian Differential Privacy}}},
  author = {Smith, Adam and Thakurta, Abhradeep},
  date = {2022-10-31},
  eprint = {2210.17520},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.17520},
  urldate = {2022-11-13},
  abstract = {We show that Gaussian Differential Privacy, a variant of differential privacy tailored to the analysis of Gaussian noise addition, composes gracefully even in the presence of a fully adaptive analyst. Such an analyst selects mechanisms (to be run on a sensitive data set) and their privacy budgets adaptively, that is, based on the answers from other mechanisms run previously on the same data set. In the language of Rogers, Roth, Ullman and Vadhan, this gives a filter for GDP with the same parameters as for nonadaptive composition.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@inproceedings{WRRW23,
  ids = {whitehouse2023fully},
  title = {Fully-{{Adaptive Composition}} in {{Differential Privacy}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Whitehouse, Justin and Ramdas, Aaditya and Rogers, Ryan and Wu, Steven},
  date = {2023-07-03},
  pages = {36990--37007},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/whitehouse23a.html},
  urldate = {2023-11-29},
  abstract = {Composition is a key feature of differential privacy. Well-known advanced composition theorems allow one to query a private database quadratically more times than basic privacy composition would permit. However, these results require that the privacy parameters of all algorithms be fixed before interacting with the data. To address this, Rogers et al. introduced fully adaptive composition, wherein both algorithms and their privacy parameters can be selected adaptively. They defined two probabilistic objects to measure privacy in adaptive composition: privacy filters, which provide differential privacy guarantees for composed interactions, and privacy odometers, time-uniform bounds on privacy loss. There are substantial gaps between advanced composition and existing filters and odometers. First, existing filters place stronger assumptions on the algorithms being composed. Second, these odometers and filters suffer from large constants, making them impractical. We construct filters that match the rates of advanced composition, including constants, despite allowing for adaptively chosen privacy parameters. En route we also derive a privacy filter for approximate zCDP. We also construct several general families of odometers. These odometers match the tightness of advanced composition at an arbitrary, preselected point in time, or at all points in time simultaneously, up to a doubly-logarithmic factor. We obtain our results by leveraging advances in martingale concentration. In sum, we show that fully adaptive privacy is obtainable at almost no loss.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}
