\section{Preliminaries}
\label{sec:preliminaries}

Differential privacy is the study of randomized algorithms (called mechanisms) $M : \cD \to \cY$ where $\cD$ denotes some space of datasets containing sensitive personal information. Differential privacy requires that an adversary should not gain much statistical advantage for inferring whether the mechanism was run on dataset $D_1 \in \cD$ or a second dataset $D_2 \in \cD$ when the datasets are close to each other in some sense. That is, the distributions of $M(D_1)$ and $M(D_2)$ should be difficult to distinguish from one another in the hypothesis testing sense and this should hold uniformly for any close datasets. The measure of closeness for datasets is generally defined by a neighbouring relation but this turns out not to be relevant to our work. In fact, our conclusions hold for an arbitrary pair of datasets. For this reason, we fix throughout a single arbitrary pair of datasets $(D_1, D_2) \in \cD \times \cD$ and view the privacy characteristics of a mechanism $M$ in terms of the hypothesis testing problem $M(D_1)$ vs. $M(D_2)$.

In our work we reason about the composition of adaptive mechanisms, which are permitted to inspect outcomes of any previously run mechanism. It is enough to define adaptivity for just two rounds of interaction as we can define more complex adaptively composed mechanisms inductively.

\begin{definition}
	Let $M_1 : \cD \to \cY_1$ be a mechanism and let $M_2 : \cD \times \cY_1 \to \cY_2$ be an adaptive mechanism. The adaptive composition of $M_1$ and $M_2$ is the mechanism
	\begin{align*}
		(M_1 \otimes M_2)(D) := (Y_1, Y_2), Y_1 \sim M_1(D), Y_2 \sim M_2(D; Y_1).
	\end{align*}
\end{definition}

It is natural in general to view the privacy characteristics of any private mechanism as a hypothesis testing problem \cite{WassermanZ10,DRS22}. As such it is desirable to furnish hypothesis testing problems $(P, Q)$ with some kind of ordering so that we can compare the privacy guarantees offered by various mechanisms. We also require an ordering so that we can make sense of natural privacy filters. Fortunately testing pairs can be equipped with a very natural order, namely the Blackwell order.

\begin{definition}
	For a pair of distributions $(P, Q)$ defined on a common probability space $\Omega$ and a second pair $(P', Q')$ defined on $\Omega'$, we say that $(P', Q')$ dominates $(P, Q)$ in the Blackwell order, written $(P, Q) \preceq (P', Q')$, if we can find a Markov kernel $\phi$ from $\Omega'$ to $\Omega$ such that $P = \phi P'$ and $Q = \phi Q'$. We write $(P, Q) \prec (P', Q')$ if $(P, Q) \preceq (P', Q')$ but not vice-versa.
	% If $(P, Q) \preceq (P', Q')$ and vice-versa, we write $(P, Q) \equiv (P', Q')$. If $(P, Q) \equiv (Q, P)$ the pair is called symmetric.
\end{definition}

Informally, the Blackwell order says that more information is available for distinguishing $P'$ and $Q'$ compared to $P$ vs $Q$. For mechanisms, we can interpret this as $(M(D_1), M(D_2)) \preceq (M'(D_1), M'(D_2))$ when $M$ can be reconstructed from $M'$ by introducing additional randomness through the kernel $\phi$. Note that the Blackwell order is a partial order.
% In general it is also the case that not all pairs are symmetric.

Instead of a carrying around a pair of distributions to represent the privacy characteristics of a mechanism, we can consolidate them into a privacy loss distribution (PLD) \cite{DworkR16}. As we will see, the PLD possesses very desirable composition properties and is a practical basis for privacy loss computations. To formalize the PLD, we require likelihood ratios. Recall that, for a distribution $P$ absolutely continuous with respect to a distribution $Q$ ($P \ll Q$) on a probability domain $\Omega$, their likelihood ratio is the Radon-Nikodym derivative $\ell(\omega) := \frac{dP}{dQ}(\omega)$. However, in some cases, we would like to consider private mechanisms that violate absolute continuity and still construct the PLD. This can be achieved by taking any measure $\nu$ on $\Omega$ such that $P, Q \ll \nu$ (e.g. $\nu := P + Q$) and setting $\ell(\omega) := \frac{dP}{d\nu}(\omega)/\frac{dQ}{d\nu}(\omega)$ ($\cdot/0 := \infty$). This construction of the likelihood ratio is unique only outside of a null-measure set\footnote{More carefully, assume $P, Q \ll \nu, \nu'$ and set $\mu := \nu + \nu'$ so that $P, Q \ll \nu, \nu' \ll \mu$. By Radon--Nikodym and the chain rule for measures, we have, for $\omega \sim P$, that $\frac{dP}{d\nu}(\omega)/\frac{dQ}{d\nu}(\omega) = \frac{dP}{d\mu}(\omega)/\frac{dQ}{d\mu}(\omega) = \frac{dP}{d\nu'}(\omega)/\frac{dQ}{d\nu'}(\omega)$ with probability $1$.}, which is enough to construct a unique PLD. For this reason, we denote by $\frac{dP}{dQ}$ the likelihood ratio of $P$ with respect to $Q$ even when $P \not\ll Q$.

\begin{definition}
	Let $P$ and $Q$ be distributions on $\Omega$. The PLD of $(P, Q)$, denoted $\pld(P \parallel Q)$, is the distribution over $\bR \cup \{\infty\}$ of $\log(\frac{dP}{dQ}(\omega)), \omega \sim P$.
\end{definition}

For shorthand, we denote by $\pld(M) := \pld(M(D_1) \parallel M(D_2))$ the PLD of a mechanism $M$. Note that a distribution can be easily recognized as a PLD as follows.

\begin{proposition}
	A distribution $L$ on $\bR \cup \{\infty\}$ is the PLD of some $(P, Q$) if and only if it satisfies $\E_{Z \sim L}[e^{-Z}] \leq 1$. In this case, one such pair is $(L, L')$ where $L'$ is the Esscher tilt of $L$, namely $dL'(z) := e^{-z} dL(z)$ for $z \in \bR$ and $L'(\{-\infty\}) := 1 - \E_{Z \sim L}[e^{-Z}]$.
\end{proposition}

PLDs also inherit the Blackwell order: For PLDs $L$ and $L'$, we say that $L \preceq L'$ if there exist pairs $(P, Q)$ and $(P', Q')$ such that $L := \pld(P \parallel Q)$, $L' = \pld(P' \parallel Q')$, and $(P, Q) \preceq (P', Q')$. It is straightforward to show that this order is well-defined regardless of the underlying representations $(P, Q)$ and $(P', Q')$.

PLDs also form a commutative monoid under convolution that is monotone in the Blackwell order.

\begin{proposition}
	\label{prop:pld_convolution_properties}
	For any PLDs $L_1, L_2, L_3, L'_1, L'_2$,
	\begin{enumerate}
		\item The convolution $L_1 \oplus L_2$ is also a PLD;
		\item The identically zero distribution $\Iddist$ is a PLD such that $L_1 \oplus \Iddist = \Iddist \oplus L_1 = L_1$;
		\item $(L_1 \oplus L_2) \oplus L_3 = L_1 \oplus (L_2 \oplus L_3)$;
        \item $L_1 \oplus L_2 = L_2 \oplus L_1$; and
		\item If $L_1 \preceq L'_1$ and $L_2 \preceq L'_2$, then $L_1 \oplus L_2 \preceq L'_1 \oplus L'_2$.
	\end{enumerate}
\end{proposition}

A highly prized property of PLDs is that the privacy characteristics of an adaptive mechanism corresponds to convolution of PLDs, provided that each component fixes a privacy budget in advance. This is important because the convolution of a long sequence of distributions can be computed very efficiently by applying the fast Fourier transform \cite{koskela2020computing}. Note that we will require more than this to understand fully adaptive composition where the privacy bound of each component is itself adaptive.

\begin{proposition}
	\label{prop:composition_convolution}
	Let $M_1 : \cD \to \cY_1$ and $M_2 : \cD \times \cY_1 \to \cY_2$ be adaptive mechanisms such that $\pld(M_1) \preceq L_1$ and $\pld(M_2(\cdot; y_1)) \preceq L_2$ for every $y_1 \in \cY_1$. Then $\pld(M_1 \otimes M_2) \preceq L_1 \oplus L_2$. Moreover, if $\pld(M_1) \preceq \pld(M'_1)$ and $\pld(M_2(\cdot; y_1)) \preceq \pld(M'_2(\cdot; y_1))$ for every $y_1 \in \cY_1$, then $\pld(M_1 \otimes M_2) \preceq \pld(M'_1 \otimes M'_2)$.
\end{proposition}

Closely related to the PLD is the hockey-stick curve: a convex reparameterization of the privacy profile of a mechanism generalizing the classical notion of $(\eps, \delta)$-DP.

\begin{definition}
	For a pair of distributions $(P, Q)$ over $\Omega$, their hockey-stick divergence at order $x \in \bR^\times := (0, \infty)$ is
	\begin{align*}
		H_x(P \parallel Q) := \sup_{E \subseteq \Omega} P(E) - xQ(E).
	\end{align*}
\end{definition}

Again, for convenience, we write $H_x(M) := H_x(M(D_1) \parallel M(D_2))$ and sometimes $h_M(x) := H_x(M)$ for the hockey-stick curve of a mechanism $M$. We will rely on hockey-stick curves in order to reason about adversarial behaviour of adaptive mechanisms. The hockey-stick curve is very closely related to the classical $(\eps, \delta)$ form of differential privacy. In particular, by construction of the hockey-stick divergence a mechanism $M$ satisfies $(\eps, \delta)$-DP exactly when $H_{e^\eps}(M) \leq \delta$. In general, hockey-stick curves are characterized by a few simple conditions.

\begin{proposition}[\cite{ZhuDW22} Lemma 9]
	\label{prop:hs_characterization}
	A curve $h : \bR^\times \to [0, 1]$ is a hockey-stick curve of some pair $(P, Q)$ if and only if $h$ is convex and decreasing such that $\lim_{x \to 0}h(x) = 1$ and $h(x) \geq 1 - x$ for all $x \in \bR^\times$.
\end{proposition}

Hockey-stick curves are also closely related to PLDs and can be fully recovered from the PLD via the following formula.

\begin{proposition}
	\label{prop:pld_to_hs}
	For any mechanism $M$ and $x \in \bR^\times$,
	\begin{align*}
		h_M(x) = \E_{Z \sim \pld(M)}[(1 - xe^{-Z})_+]
	\end{align*}
	where $(t)_+ := \max\{0, t\}$.
\end{proposition}

\begin{proof}
	For any pair $(P, Q)$ defined on $\Omega$ with likelihood ratio $\ell$, we have that
	\begin{align*}
		H_x(P \parallel Q)
			& = \sup_{E \subseteq \Omega} P(E) - xQ(E) \\
			& = \sup_{E \subseteq \Omega} \int_E \frac{d(P - xQ)}{dP}(\omega) \, dP(\omega) \\
			& = \sup_{E \subseteq \Omega} \int_E 1 - x/\frac{dP}{dQ}(\omega) \, dP(\omega) \\
			& = \int_\Omega \left(1 - x/\frac{dP}{dQ}(\omega)\right)_+ \, dP(\omega) \\
			& = \int_\Omega (1 - xe^{-\log(\ell(\omega))})_+ \, dP(\omega) \\
			& = \E_{Z \sim \pld(P \parallel Q)}[(1 - xe^{-Z})_+].
	\end{align*}
\end{proof}

For convenience, we will sometimes write $h_L$ to denote the unique hockey-stick curve associated to a PLD $L$, namely $h_L(x) := \E_{Z \sim L}[(1 - xe^{-Z})_+]$. Moreover, like PLDs, hockey-stick curves also capture adaptive composition in a natural way.
% Doing so computationally involves expensive numerical integration and is not generally practical.
% \todo{explain that it's useful anyway 'cause the formula is more flexible than for plds}

\begin{proposition}
	\label{prop:hockey_stick_composition}
	Let $M_1 : \cD \to \cY_1$ and $M_2 : \cD \times \cY_1 \to \cY_2$ be adaptive mechanisms and let $\ell^1 := \frac{dM_1(D_1)}{dM_1(D_2)}$ denote the likelihood ratio for $M_1$. Then
	\begin{align*}
		h_{M_1 \otimes M_2}(x) = \E_{Y_1 \sim M_1(D_1)}[h_{M_2(\cdot; Y_1)}(x/\ell^1(Y_1))].
	\end{align*}
\end{proposition}

\begin{proof}
	Let $\ell$ denote the likelihood ratio for $M_1 \otimes M_2$ and let $\ell^2_{y_1}$ denote the likelihood ratio for $M_2(\cdot; y_1)$. By Bayes' rule, we have $\ell(y_1, y_2) = \ell^1(y_1) \cdot \ell^2_{y_1}(y_2)$, so we have
	\begin{align*}
		\pld(M_1 \otimes M_2) \equiv \log(\ell^1(Y_1)) + \log(\ell^2_{Y_1}(Y_2)), Y_1 \sim M_1(D_1), Y_2 \sim M_2(D_1; Y_1).
	\end{align*}
	By \Cref{prop:pld_to_hs} and the law of total expectation, it follows that
	\begin{align*}
		h_{M_1 \otimes M_2}(x)
			& = \E_{Z \sim \pld(M_1 \otimes M_2)}[
						(1 - xe^{-Z})_+
					] \\
			& = \E_{Y_1 \sim M_1(D_1)}[\E_{Y_2 \sim M_2(D_1; Y_1)}[
						(1 - xe^{-(\log(\ell^1(Y_1)) + \log(\ell^2_{Y_1}(Y_2)))})_+
					]] \\
			& = \E_{Y_1 \sim M_1(D_1)}[\E_{Y_2 \sim M_2(D_1; Y_1)}[
						(1 - x/\ell^1(Y_1) \cdot e^{-\log(\ell^2_{Y_1}(Y_2))})_+
					]] \\
			& = \E_{Y_1 \sim M_1(D_1)}[\E_{Z_2 \sim \pld(M_2(\cdot; Y_1)}[
						(1 - x/\ell^1(Y_1) \cdot e^{-Z_2})_+
					]] \\
			& = \E_{Y_1 \sim M_1(D_1)}[h_{M_2(\cdot; Y_1)}(x/\ell^1(Y_1))].
	\end{align*}
\end{proof}

Our last characterization of privacy is given the Type I/Type II error tradeoff curve, also known as the $f$-DP framework of privacy \cite{DongRS19}.

\begin{definition}
	Let $(P, Q)$ be any distributions on $\Omega$. For any hypothesis test $\phi : \Omega \to \{P, Q\}$ for distinguishing $P$ and $Q$, we call $\alpha_\phi := \bP_{\omega \sim P}[\phi(\omega) = Q]$ its Type I error and $\beta_\phi := \bP_{\omega \sim Q}[\phi(\omega) = P]$ its Type II error. The tradeoff curve for $(P, Q)$ is defined as
	\begin{align*}
		T_\alpha(P \parallel Q) := \inf_{\phi} \{\beta_\phi : \alpha_\phi \leq \alpha\}.
	\end{align*}
\end{definition}

For convenience, we sometimes write the tradeoff curve of a mechanism $M$ with respect to the fixed dataset pair $(D_1, D_2)$ as $T_\alpha(M) := T_\alpha(M(D_1) \parallel M(D_2))$ or sometimes as $\tau_M(\alpha) := T_\alpha(M)$ where appropriate. In general, we can characterize valid tradeoff curves as follows.

\begin{proposition}[\cite{DongRS19} Proposition 2.2]
	\label{prop:tradeoff_characterization}
	A curve $\tau : [0, 1] \to [0, 1]$ is a tradeoff curve for some testing problem $(P, Q)$ exactly when $\tau$ is continuous, convex, decreasing, and $\tau(\alpha) \leq 1 - \alpha$ for all $\alpha \in [0, 1]$.
\end{proposition}

Like hockey-stick curves, it is enough to understand the PLD of a testing problem in order to obtain its tradeoff curve. The following formula can be derived by the well-known Neyman--Pearson lemma.

\begin{proposition}
	\label{prop:pld_to_tradeoff}
	Let $(P, Q)$ be a pair of distributions with tradeoff curve $\tau(\alpha) := T_\alpha(P \parallel Q)$ and let $F$ denote the CDF of $\pld(P \parallel Q)$. Then
	\begin{align*}
		\tau(\alpha) = \mathbb{E}_{Z \sim \pld(P \parallel Q)}[1(F(Z) > \alpha) \cdot e^{-Z}]
	\end{align*}
\end{proposition}

It is common to represent a pair of distributions by either their hockey-stick curve or their tradeoff function. We show that there is a natural link between these representations via inversion and convex conjugacy. For a tradeoff function $\tau$ of a testing problem $(P, Q)$, we write $\tau^{-1}(\beta) := \inf\{\alpha \in [0, 1] : \tau(\alpha) \leq \beta\}$ for its inverse, namely the tradeoff curve for the testing problem $(Q, P)$. For a convex function $g$, we denote its convex conjugate by $g^*(y) := \sup_{x \in \mathbb{R}} xy - g(x)$. Note that we can extend tradeoff functions to the real-line by setting them to $\infty$ outside their support $[0, 1]$. Note that a special case of the following formula appears in \cite{DongRS19} (Prop 2.12) for symmetric tradeoff functions but this generalization is new to the best of our knowledge.

\begin{proposition}
	\label{prop:tradeoff_to_hs}
	Let $(P, Q)$ be a pair of distributions with tradeoff function $\tau(\alpha) := T_\alpha(P \parallel Q)$ and hockey-stick curve $h(x) := H_x(P \parallel Q)$. Then
	\begin{align*}
		h(x) = 1 + (\tau^{-1})^*(-x)
	\end{align*}
\end{proposition}

% \matt{note: for now the proof skips lots of little details, mostly relating to edge cases involving point masses and how it affects $\tau^{-1}$, $F_L^{-1}$ etc.}

\begin{proof}
	First, let $L$ denote the PLD of $(P, Q)$ and let $F$ be its CDF. By \Cref{prop:pld_to_tradeoff},
	\begin{align*}
		\tau(\alpha) = \mathbb{E}_{Z \sim L}[1(Z > F^{-1}(\alpha)) \cdot e^{-Z}].
	\end{align*}
	Therefore, by \Cref{prop:pld_to_hs}
	\begin{align*}
		h(x)
			& = \mathbb{E}_{Z \sim L}[(1 - x e^{-Z})_+] \\
			& = \mathbb{E}_{Z \sim L}[1(Z > \log(x)) \cdot \underbrace{(1 - x e^{-Z})}_{> 0 \iff Z > \log(x)}] \\
			& = \sup_{\varepsilon \in \mathbb{R}}\mathbb{E}_{Z \sim L}[1(Z > \varepsilon) \cdot (1 - x e^{-Z})] \\
			& = \sup_{\alpha \in [0, 1]}\mathbb{E}_{Z \sim L}[1(Z > F^{-1}(\alpha)) \cdot (1 - x e^{-Z})] \\
			& = \sup_{\alpha \in [0, 1]} 1 - \alpha - x\tau(\alpha) \\
			& = \sup_{\beta \in [0, 1]} 1 - \tau^{-1}(\beta) - x \beta \tag{$\alpha = \tau^{-1}(\beta)$} \\
			& = 1 + (\tau^{-1})^*(-x).
	\end{align*}
\end{proof}

It follows as a natural consequence that the Blackwell order on pairs, the ordering induced by the tradeoff function, as well as the hockey-stick induced ordering are all equivalent.

\begin{proposition}
	\label{prop:f-hs-equivalence}
	Let $(P, Q)$ and $(P', Q')$ be pairs of distributions with tradeoff functions $\tau$ and $\tau'$ respectively as well as hockey-stick curves $h$ and $h'$ respectively. The following are equivalent
	\begin{enumerate}[(i)]
		\item $(P, Q) \preceq (P', Q')$
		\item $\tau \succeq \tau'$
		\item $h \preceq h'$
	\end{enumerate}
\end{proposition}

The equivalence of (i) and (ii) is given by a celebrated theorem of Blackwell \cite[Theorem 10]{Bla51}. As for the equivalence of (ii) and (iii), this now follows by applying Fenchelâ€“Moreau duality to \Cref{prop:tradeoff_to_hs} and recalling that convex conjugation is an order-reversing operation.

As a consequence, it turns out that PLDs endowed with the Blackwell order possess the same remarkable completeness property that enables analysis in the real numbers.

\begin{proposition}\label{prop:sup-conv}
	For any non-empty family of PLDs $\mathcal{L}$ dominated by at least one PLD, there exists a unique PLD $\sup \mathcal{L}$ that dominates $\mathcal{L}$ but is dominated by every upper bound for $\mathcal{L}$. In this case,
	\begin{enumerate}[(i)]
		\item $h_{\sup \mathcal{L}}(x) = \sup\{h_L(x) : L \in \mathcal{L}\}$
		\item $\tau_{\sup \mathcal{L}} = \conv\{\tau_L : L \in \mathcal{L}\}$
	\end{enumerate}
	where $(\conv{\cF})(\alpha) := \sup\{f(\alpha) : f \text{ is convex}, f \preceq \cF\}$ denotes the lower convex envelope.
\end{proposition}

This result follows immediately from \Cref{prop:f-hs-equivalence} and from noticing that the properties characterizing hockey-stick curves (see \Cref{prop:hs_characterization}) are all closed under pointwise suprema. Part (ii) follows from noticing that all of the properties of tradeoff curves (see \Cref{prop:tradeoff_characterization}) are also closed under the lower convex envelope operation.

We rely on the supremum extensively in order to reason about natural privacy filters. In particular, the former characterization says that the supremum of PLDs is given by a pointwise supremum in the space of hockey-stick curves, which will be particularly useful for showing our main result. As far as we are aware, the supremum property of PLDs has not actually been documented in the literature.
